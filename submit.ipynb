{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file can be submitted to Kaggle and it will successfully submit an attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import datetime # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# I like to disable my Notebook Warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Notebook Configuration...\n",
    "\n",
    "# Amount of data we want to load into the Model...\n",
    "DATA_ROWS = None\n",
    "# Dataframe, the amount of rows and cols to visualize...\n",
    "NROWS = 50\n",
    "NCOLS = 15\n",
    "# Main data location path...\n",
    "BASE_PATH = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Configure notebook display settings to only use 2 decimal places, tables look nicer.\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_columns', NCOLS) \n",
    "pd.set_option('display.max_rows', NROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, OneHotEncoder, OrdinalEncoder\n",
    "def get_data(fill_values  = [3, 2, 0, 3, 0], get_test=False):\n",
    "    # Load the CSV information into a Pandas DataFrame...\n",
    "    trn_data = pd.read_feather('../input/parquet-files-amexdefault-prediction/train_data.ftr')\n",
    "    trn_lbls = pd.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv').set_index('customer_ID')\n",
    "    if(get_test):\n",
    "        tst_data = pd.read_feather('../input/parquet-files-amexdefault-prediction/test_data.ftr')\n",
    "\n",
    "    #%%time\n",
    "    if(get_test):\n",
    "        sub = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n",
    "    else:\n",
    "        sub = \"Get test off\"\n",
    "    ## 6.1 Training Dataset...\n",
    "    print(trn_data[:5])\n",
    "    # We have 458913 customers. and we have 458913 train labels...\n",
    "\n",
    "    # Calculates the amount of information by costumer or records available...\n",
    "    trn_num_statements = trn_data.groupby('customer_ID').size().sort_index()\n",
    "    print(trn_data)\n",
    "    # Create a new dataset based on aggregated information\n",
    "    trn_agg_data = (trn_data\n",
    "                    .groupby('customer_ID')\n",
    "                    .tail(1)\n",
    "                    .set_index('customer_ID', drop=True)\n",
    "                    .sort_index()\n",
    "                    .drop(['S_2'], axis='columns'))\n",
    "    del trn_data\n",
    "    # Merge the labels from the labels dataframe\n",
    "    trn_agg_data['target'] = trn_lbls.target\n",
    "    del trn_lbls\n",
    "    trn_agg_data['num_statements'] = trn_num_statements\n",
    "    del trn_num_statements\n",
    "    \n",
    "    trn_agg_data.reset_index(inplace = True, drop = True) # forget the customer_IDs\n",
    "\n",
    "    ## 6.2 Test Dataset...\n",
    "\n",
    "    # Calculates the amount of information by costumer or records available...\n",
    "    if(get_test):\n",
    "        tst_num_statements = tst_data.groupby('customer_ID').size().sort_index()\n",
    "\n",
    "        # Create a new dataset based on aggregated information\n",
    "        tst_agg_data = (tst_data\n",
    "                        .groupby('customer_ID')\n",
    "                        .tail(1)\n",
    "                        .set_index('customer_ID', drop=True)\n",
    "                        .sort_index()\n",
    "                        .drop(['S_2'], axis='columns'))\n",
    "        del tst_data\n",
    "        # Merge the labels from the labels dataframe\n",
    "        tst_agg_data['num_statements'] = tst_num_statements\n",
    "        del tst_num_statements\n",
    "        tst_agg_data.reset_index(inplace = True, drop = True) # forget the customer_IDs\n",
    "\n",
    "    # 7.0 Label / One-Hot Encoding the Categorical Variables...\n",
    "\n",
    "    ## 7.1 One Hot Encoding Configuration...\n",
    "\n",
    "    # One-hot Encoding Configuration\n",
    "    cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "    #trn_agg_data[cat_features] = trn_agg_data[cat_features].astype(object)\n",
    "    trn_not_cat_features = [f for f in trn_agg_data.columns if f not in cat_features]\n",
    "    if(get_test):\n",
    "        tst_not_cat_features = [f for f in tst_agg_data.columns if f not in cat_features]\n",
    "\n",
    "    #encoder = OneHotEncoder(drop = 'first', sparse = False, dtype = np.float32, handle_unknown = 'ignore')\n",
    "    encoder = OrdinalEncoder()\n",
    "    trn_encoded_features = encoder.fit_transform(trn_agg_data[cat_features])\n",
    "    #feat_names = list(encoder.get_feature_names())\n",
    "\n",
    "    ## 7.2 Train Dataset One Hot Encoding...\n",
    "\n",
    "    # One-hot Encoding\n",
    "    trn_encoded_features = pd.DataFrame(trn_encoded_features)\n",
    "    #trn_encoded_features.columns = feat_names\n",
    "\n",
    "    trn_agg_data = pd.concat([trn_agg_data[trn_not_cat_features], trn_encoded_features], axis = 1)\n",
    "\n",
    "    ## 7.3 Test Dataset One-Hot Encoding...\n",
    "    if(get_test):\n",
    "        # One-hot Encoding\n",
    "        tst_encoded_features = encoder.transform(tst_agg_data[cat_features])\n",
    "        tst_encoded_features = pd.DataFrame(tst_encoded_features)\n",
    "        #tst_encoded_features.columns = feat_names\n",
    "\n",
    "        tst_agg_data = pd.concat([tst_agg_data[tst_not_cat_features], tst_encoded_features], axis = 1)\n",
    "        tst_agg_data.head()\n",
    "\n",
    "    features = [f for f in trn_agg_data.columns if f != 'target' and f != 'customer_ID']\n",
    "    \n",
    "    c = trn_agg_data[features].columns.str\n",
    "    cs = [c.startswith('S_', False), c.startswith('P_', False), c.startswith('B_', False), c.startswith('R_', False), c.startswith('D_', False)]\n",
    "    cs = [trn_agg_data[features].columns[c_i] for c_i in cs]\n",
    "    print(cs)\n",
    "    #\n",
    "    # Impute missing values\n",
    "    # Old fill type values\n",
    "    for i_fill in range(len(fill_values)):\n",
    "        if(fill_values[i_fill]==0):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = 0, inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = 0, inplace = True)\n",
    "        elif(fill_values[i_fill]==1):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = np.nanmean(trn_agg_data[cs[i_fill]]), inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = np.nanmean(trn_agg_data[cs[i_fill]]), inplace = True)         \n",
    "        elif(fill_values[i_fill]==2):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .25), inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .25), inplace = True)       \n",
    "        elif(fill_values[i_fill]==3):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .75), inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .75), inplace = True)            \n",
    "    #Fill all others\n",
    "    trn_agg_data.fillna(value = 0, inplace = True)\n",
    "    if(get_test):\n",
    "        tst_agg_data.fillna(value = 0, inplace = True)\n",
    "\n",
    "    # 10.0 NN Development\n",
    "\n",
    "    # Release some memory by deleting the original DataFrames...\n",
    "    gc.collect()\n",
    "    if(get_test==False):\n",
    "        tst_agg_data = 0\n",
    "    return trn_agg_data, tst_agg_data, features, sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, InputLayer, Add, BatchNormalization, Dropout, Concatenate, Reshape, Conv1D, Flatten\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# From https://www.kaggle.com/code/inversion/amex-competition-metric-python\n",
    "import torch\n",
    "\n",
    "def amex_metric(y_true, y_pred, return_components=False) -> float:\n",
    "    \"\"\"Amex metric for ndarrays\"\"\"\n",
    "    def top_four_percent_captured(df) -> float:\n",
    "        \"\"\"Corresponds to the recall for a threshold of 4 %\"\"\"\n",
    "        \n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "    \n",
    "    \n",
    "    def weighted_gini(df) -> float:\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    \n",
    "    def normalized_weighted_gini(df) -> float:\n",
    "        \"\"\"Corresponds to 2 * AUC - 1\"\"\"\n",
    "        \n",
    "        df2 = pd.DataFrame({'target': df.target, 'prediction': df.target})\n",
    "        df2.sort_values('prediction', ascending=False, inplace=True)\n",
    "        return weighted_gini(df) / weighted_gini(df2)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({'target': tf.experimental.numpy.ravel(y_true), 'prediction': tf.experimental.numpy.ravel(y_pred)})\n",
    "    df.sort_values('prediction', ascending=False, inplace=True)\n",
    "    g = normalized_weighted_gini(df)\n",
    "    d = top_four_percent_captured(df)\n",
    "\n",
    "    if return_components: return g, d, 0.5 * (g + d)\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def nn_model():\n",
    "    '''\n",
    "    '''\n",
    "    regularization = 4e-4\n",
    "    activation_func = 'swish'\n",
    "    inputs = Input(shape = (len(features)))\n",
    "    \n",
    "    x = Dense(256, \n",
    "              #use_bias  = True, \n",
    "              kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "              activation = activation_func)(inputs)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(64, \n",
    "              #use_bias  = True, \n",
    "              kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "              activation = activation_func)(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(64, \n",
    "          #use_bias  = True, \n",
    "          kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "          activation = activation_func)(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(32, \n",
    "              #use_bias  = True, \n",
    "              kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "              activation = activation_func)(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(1, \n",
    "              #use_bias  = True, \n",
    "              #kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "              activation = 'sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Defining model parameters...\n",
    "BATCH_SIZE         = 256\n",
    "EPOCHS             = 170 \n",
    "EPOCHS_COSINEDECAY = 170\n",
    "DIAGRAMS           = True\n",
    "USE_PLATEAU        = False\n",
    "INFERENCE          = False\n",
    "VERBOSE            = 0 \n",
    "TARGET             = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def nn_model(features, regularization = 4e-4):\n",
    "    activation_func = 'swish'\n",
    "    inputs = Input(shape = (len(features)))\n",
    "\n",
    "    x0 = Dense(256,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "               activation = activation_func)(inputs)\n",
    "    x1 = Dense(128,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "               activation = activation_func)(x0)\n",
    "    x1 = Dense(64,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "               activation = activation_func)(x1)\n",
    "    x1 = Dense(32,\n",
    "           kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "           activation = activation_func)(x1)\n",
    "    \n",
    "    x1 = Concatenate()([x1, x0])\n",
    "    x1 = Dropout(0.1)(x1)\n",
    "    \n",
    "    x1 = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(regularization),activation=activation_func,)(x1)\n",
    "    \n",
    "    x1 = Dense(1, \n",
    "              #kernel_regularizer=tf.keras.regularizers.l2(regularization),\n",
    "              activation='sigmoid')(x1)\n",
    "    \n",
    "    model = Model(inputs, x1)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %%time\n",
    "# Defining model training function...\n",
    "history_list = []\n",
    "def fit_model(X_train, y_train, X_val, y_val, model, run = 0):\n",
    "    '''\n",
    "    '''\n",
    "    lr_start = 0.01\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    epochs = EPOCHS    \n",
    "    lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, patience = 4, verbose = VERBOSE)\n",
    "    es = EarlyStopping(monitor = 'val_loss',patience = 12, verbose = 1, mode = 'min', restore_best_weights = True)\n",
    "    tm = tf.keras.callbacks.TerminateOnNaN()\n",
    "    callbacks = [lr, es, tm]\n",
    "    \n",
    "    # Cosine Learning Rate Decay\n",
    "    if USE_PLATEAU == False:\n",
    "        epochs = EPOCHS_COSINEDECAY\n",
    "        lr_end = 0.0002\n",
    "\n",
    "        def cosine_decay(epoch):\n",
    "            if epochs > 1:\n",
    "                w = (1 + math.cos(epoch / (epochs - 1) * math.pi)) / 2\n",
    "            else:\n",
    "                w = 1\n",
    "            return w * lr_start + (1 - w) * lr_end\n",
    "        \n",
    "        lr = LearningRateScheduler(cosine_decay, verbose = 0)\n",
    "        callbacks = [lr, tm]\n",
    "    \n",
    "    # Model Initialization...\n",
    "    #model = nn_model(features)\n",
    "    optimizer_func = tf.keras.optimizers.Adam(learning_rate = lr_start)\n",
    "    loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(optimizer = optimizer_func, loss = loss_func, metrics = [amex_metric])\n",
    "    \n",
    "    \n",
    "    X_val = scaler.transform(X_val)\n",
    "    validation_data = (X_val, y_val)\n",
    "    \n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data = validation_data, \n",
    "                        epochs          = epochs,\n",
    "                        verbose         = VERBOSE,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        shuffle         = True,\n",
    "                        callbacks       = callbacks\n",
    "                       )\n",
    "    print(\"Model fitted\")\n",
    "    history_list.append(history.history)\n",
    "        \n",
    "    print(f'Training Loss: {history_list[-1][\"loss\"][-1]:.5f}, Validation Loss: {history_list[-1][\"val_loss\"][-1]:.5f}')\n",
    "    callbacks, es, lr, tm, history = None, None, None, None, None\n",
    "    \n",
    "    \n",
    "    y_val_pred = model.predict(X_val, batch_size = BATCH_SIZE, verbose = VERBOSE).ravel()\n",
    "    amex_score = amex_metric(y_val.values, y_val_pred, return_components = False)\n",
    "    \n",
    "    print(f'Fold {run} | {str(datetime.datetime.now() - start_time)[-12:-7]}'\n",
    "          f'| Amex Score: {amex_score:.5f}')\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    #score_list.append(amex_score)\n",
    "    \n",
    "    tst_data_scaled = scaler.transform(tst_agg_data[features])\n",
    "    tst_pred = model.predict(tst_data_scaled)\n",
    "    predictions.append(tst_pred)\n",
    "    print(amex_score)\n",
    "    print(history)\n",
    "    return amex_score, history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import math\n",
    "def train(trn_agg_data, features, model):\n",
    "    score_list = []\n",
    "    kf = KFold(n_splits = 5)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(trn_agg_data)):\n",
    "        X_train, X_val = trn_agg_data.iloc[trn_idx][features], trn_agg_data.iloc[val_idx][features]\n",
    "        y_train, y_val = trn_agg_data.iloc[trn_idx][TARGET], trn_agg_data.iloc[val_idx][TARGET]\n",
    "        print(\"Fold\",fold)\n",
    "        score, history = fit_model(X_train, y_train, X_val, y_val, model)\n",
    "        print(\"In loop (train):\", history)\n",
    "        score_list.append(score)\n",
    "    current_score = np.mean(score_list)\n",
    "    print(f'OOF AUC: {current_score:.5f}')\n",
    "    print(history)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import math\n",
    "\n",
    "# Create empty lists to store NN information...\n",
    "trn_agg_data, tst_agg_data, features, sub = get_data(get_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []\n",
    "predictions = []\n",
    "history = train(trn_agg_data, features, nn_model(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub['prediction'] = np.array(predictions).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub.to_csv('my_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
