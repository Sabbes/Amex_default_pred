{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Loading Model Libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/amex-default-prediction/train_labels.csv\n",
      "/kaggle/input/parquet-files-amexdefault-prediction/train_data.ftr\n",
      "/kaggle/input/parquet-files-amexdefault-prediction/test_data.ftr\n",
      "CPU times: user 13.4 ms, sys: 921 µs, total: 14.3 ms\n",
      "Wall time: 9.24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import datetime # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Setting the Notebook Parameters and Default Configuration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31 µs, sys: 5 µs, total: 36 µs\n",
      "Wall time: 41.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# I like to disable my Notebook Warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Notebook Configuration...\n",
    "\n",
    "# Amount of data we want to load into the Model...\n",
    "DATA_ROWS = None\n",
    "# Dataframe, the amount of rows and cols to visualize...\n",
    "NROWS = 50\n",
    "NCOLS = 15\n",
    "# Main data location path...\n",
    "BASE_PATH = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 96 µs, sys: 17 µs, total: 113 µs\n",
      "Wall time: 120 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Configure notebook display settings to only use 2 decimal places, tables look nicer.\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_columns', NCOLS) \n",
    "pd.set_option('display.max_rows', NROWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Loading the Dataset Information (Using Feather)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, OneHotEncoder, OrdinalEncoder\n",
    "def get_data(fill_values, get_test=False):\n",
    "    # Load the CSV information into a Pandas DataFrame...\n",
    "    trn_data = pd.read_feather('../input/parquet-files-amexdefault-prediction/train_data.ftr')\n",
    "    trn_lbls = pd.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv').set_index('customer_ID')\n",
    "    if(get_test):\n",
    "        tst_data = pd.read_feather('../input/parquet-files-amexdefault-prediction/test_data.ftr')\n",
    "\n",
    "    #%%time\n",
    "    #sub = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n",
    "\n",
    "    ## 6.1 Training Dataset...\n",
    "\n",
    "    # We have 458913 customers. and we have 458913 train labels...\n",
    "\n",
    "    # Calculates the amount of information by costumer or records available...\n",
    "    trn_num_statements = trn_data.groupby('customer_ID').size().sort_index()\n",
    "\n",
    "    # Create a new dataset based on aggregated information\n",
    "    trn_agg_data = (trn_data\n",
    "                    .groupby('customer_ID')\n",
    "                    .tail(1)\n",
    "                    .set_index('customer_ID', drop=True)\n",
    "                    .sort_index()\n",
    "                    .drop(['S_2'], axis='columns'))\n",
    "    del trn_data\n",
    "    # Merge the labels from the labels dataframe\n",
    "    trn_agg_data['target'] = trn_lbls.target\n",
    "    del trn_lbls\n",
    "    trn_agg_data['num_statements'] = trn_num_statements\n",
    "    del trn_num_statements\n",
    "    \n",
    "    trn_agg_data.reset_index(inplace = True, drop = True) # forget the customer_IDs\n",
    "\n",
    "    ## 6.2 Test Dataset...\n",
    "\n",
    "    # Calculates the amount of information by costumer or records available...\n",
    "    if(get_test):\n",
    "        tst_num_statements = tst_data.groupby('customer_ID').size().sort_index()\n",
    "\n",
    "        # Create a new dataset based on aggregated information\n",
    "        tst_agg_data = (tst_data\n",
    "                        .groupby('customer_ID')\n",
    "                        .tail(1)\n",
    "                        .set_index('customer_ID', drop=True)\n",
    "                        .sort_index()\n",
    "                        .drop(['S_2'], axis='columns'))\n",
    "        del tst_data\n",
    "        # Merge the labels from the labels dataframe\n",
    "        tst_agg_data['num_statements'] = tst_num_statements\n",
    "        del tst_num_statements\n",
    "        tst_agg_data.reset_index(inplace = True, drop = True) # forget the customer_IDs\n",
    "\n",
    "    # 7.0 Label / One-Hot Encoding the Categorical Variables...\n",
    "\n",
    "    ## 7.1 One Hot Encoding Configuration...\n",
    "\n",
    "    # One-hot Encoding Configuration\n",
    "    cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "    #trn_agg_data[cat_features] = trn_agg_data[cat_features].astype(object)\n",
    "    trn_not_cat_features = [f for f in trn_agg_data.columns if f not in cat_features]\n",
    "    if(get_test):\n",
    "        tst_not_cat_features = [f for f in tst_agg_data.columns if f not in cat_features]\n",
    "\n",
    "    #encoder = OneHotEncoder(drop = 'first', sparse = False, dtype = np.float32, handle_unknown = 'ignore')\n",
    "    encoder = OrdinalEncoder()\n",
    "    trn_encoded_features = encoder.fit_transform(trn_agg_data[cat_features])\n",
    "    #feat_names = list(encoder.get_feature_names())\n",
    "\n",
    "    ## 7.2 Train Dataset One Hot Encoding...\n",
    "\n",
    "    # One-hot Encoding\n",
    "    trn_encoded_features = pd.DataFrame(trn_encoded_features)\n",
    "    #trn_encoded_features.columns = feat_names\n",
    "\n",
    "    trn_agg_data = pd.concat([trn_agg_data[trn_not_cat_features], trn_encoded_features], axis = 1)\n",
    "\n",
    "    ## 7.3 Test Dataset One-Hot Encoding...\n",
    "    if(get_test):\n",
    "        # One-hot Encoding\n",
    "        tst_encoded_features = encoder.transform(tst_agg_data[cat_features])\n",
    "        tst_encoded_features = pd.DataFrame(tst_encoded_features)\n",
    "        #tst_encoded_features.columns = feat_names\n",
    "\n",
    "        tst_agg_data = pd.concat([tst_agg_data[tst_not_cat_features], tst_encoded_features], axis = 1)\n",
    "        tst_agg_data.head()\n",
    "\n",
    "    features = [f for f in trn_agg_data.columns if f != 'target' and f != 'customer_ID']\n",
    "    \n",
    "    c = trn_agg_data[features].columns.str\n",
    "    cs = [c.startswith('S_', False), c.startswith('P_', False), c.startswith('B_', False), c.startswith('R_', False), c.startswith('D_', False)]\n",
    "    cs = [trn_agg_data[features].columns[c_i] for c_i in cs]\n",
    "    #\n",
    "    # Impute missing values\n",
    "    # Old fill type values\n",
    "    for i_fill in range(len(fill_values)):\n",
    "        if(fill_values[i_fill]==0):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = 0, inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = 0, inplace = True)\n",
    "        elif(fill_values[i_fill]==1):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = np.nanmean(trn_agg_data[cs[i_fill]]), inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = np.nanmean(trn_agg_data[cs[i_fill]]), inplace = True)         \n",
    "        elif(fill_values[i_fill]==2):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .25), inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .25), inplace = True)       \n",
    "        elif(fill_values[i_fill]==3):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .75), inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .75), inplace = True)            \n",
    "    #Fill all others\n",
    "    trn_agg_data.fillna(value = 0, inplace = True)\n",
    "    if(get_test):\n",
    "        tst_agg_data.fillna(value = 0, inplace = True)\n",
    "\n",
    "    # 10.0 NN Development\n",
    "\n",
    "    # Release some memory by deleting the original DataFrames...\n",
    "    gc.collect()\n",
    "    if(get_test==False):\n",
    "        tst_agg_data = 0\n",
    "    return trn_agg_data, tst_agg_data, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Loading Specific Model Libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.66 s, sys: 284 ms, total: 1.94 s\n",
      "Wall time: 2.08 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, InputLayer, Add, BatchNormalization, Dropout, Concatenate, Reshape, Conv1D, Flatten\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Amex Metric, Function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1 µs, total: 7 µs\n",
      "Wall time: 13.1 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# From https://www.kaggle.com/code/inversion/amex-competition-metric-python\n",
    "\n",
    "def amex_metric(y_true, y_pred, return_components=False) -> float:\n",
    "    \"\"\"Amex metric for ndarrays\"\"\"\n",
    "    \n",
    "    def top_four_percent_captured(df) -> float:\n",
    "        \"\"\"Corresponds to the recall for a threshold of 4 %\"\"\"\n",
    "        \n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "    \n",
    "    \n",
    "    def weighted_gini(df) -> float:\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    \n",
    "    def normalized_weighted_gini(df) -> float:\n",
    "        \"\"\"Corresponds to 2 * AUC - 1\"\"\"\n",
    "        \n",
    "        df2 = pd.DataFrame({'target': df.target, 'prediction': df.target})\n",
    "        df2.sort_values('prediction', ascending=False, inplace=True)\n",
    "        return weighted_gini(df) / weighted_gini(df2)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({'target': y_true.ravel(), 'prediction': y_pred.ravel()})\n",
    "    df.sort_values('prediction', ascending=False, inplace=True)\n",
    "    g = normalized_weighted_gini(df)\n",
    "    d = top_four_percent_captured(df)\n",
    "\n",
    "    if return_components: return g, d, 0.5 * (g + d)\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Defining the NN Model Architecture..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.1 Architecture 01, Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def nn_model():\n",
    "    '''\n",
    "    '''\n",
    "    regularization = 4e-4\n",
    "    activation_func = 'swish'\n",
    "    inputs = Input(shape = (len(features)))\n",
    "    \n",
    "    x = Dense(256, \n",
    "              #use_bias  = True, \n",
    "              kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "              activation = activation_func)(inputs)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(64, \n",
    "              #use_bias  = True, \n",
    "              kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "              activation = activation_func)(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(64, \n",
    "          #use_bias  = True, \n",
    "          kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "          activation = activation_func)(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(32, \n",
    "              #use_bias  = True, \n",
    "              kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "              activation = activation_func)(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(1, \n",
    "              #use_bias  = True, \n",
    "              #kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "              activation = 'sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.2 Architecture 02, Concatenated NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Defining Model Training Parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Defining model parameters...\n",
    "BATCH_SIZE         = 256\n",
    "EPOCHS             = 20 \n",
    "EPOCHS_COSINEDECAY = 20\n",
    "DIAGRAMS           = True\n",
    "USE_PLATEAU        = False\n",
    "INFERENCE          = False\n",
    "VERBOSE            = 0 \n",
    "TARGET             = 'target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def nn_model(features, regularization = 4e-4):\n",
    "    activation_func = 'swish'\n",
    "    inputs = Input(shape = (len(features)))\n",
    "\n",
    "    x0 = Dense(256,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "               activation = activation_func)(inputs)\n",
    "    x1 = Dense(128,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "               activation = activation_func)(x0)\n",
    "    x1 = Dense(64,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "               activation = activation_func)(x1)\n",
    "    x1 = Dense(32,\n",
    "           kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "           activation = activation_func)(x1)\n",
    "    \n",
    "    x1 = Concatenate()([x1, x0])\n",
    "    x1 = Dropout(0.1)(x1)\n",
    "    \n",
    "    x1 = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(regularization),activation=activation_func,)(x1)\n",
    "    \n",
    "    x1 = Dense(1, \n",
    "              #kernel_regularizer=tf.keras.regularizers.l2(regularization),\n",
    "              activation='sigmoid')(x1)\n",
    "    \n",
    "    model = Model(inputs, x1)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def nn_model_2(features):\n",
    "    regularization = 4e-4\n",
    "    activation_func = 'swish'\n",
    "    inputs = Input(shape = (len(features)))\n",
    "    #added layer\n",
    "    #---\n",
    "    xneg1 = Dense(512, kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "               activation = activation_func)(inputs)\n",
    "    xneg1 = Dropout(0.1)(xneg1)\n",
    "    #---\n",
    "    \n",
    "    x0 = Dense(256,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "               activation = activation_func)(xneg1)\n",
    "    x1 = Dense(128,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "               activation = activation_func)(x0)\n",
    "    x1 = Dense(64,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "               activation = activation_func)(x1)\n",
    "    x1 = Dense(32,\n",
    "           kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "           activation = activation_func)(x1)\n",
    "    \n",
    "    x1 = Concatenate()([x1, x0])\n",
    "    x1 = Dropout(0.1)(x1)\n",
    "    \n",
    "    x1 = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(regularization),activation=activation_func,)(x1)\n",
    "    \n",
    "    x1 = Dense(1, \n",
    "              #kernel_regularizer=tf.keras.regularizers.l2(regularization),\n",
    "              activation='sigmoid')(x1)\n",
    "    \n",
    "    model = Model(inputs, x1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Defining the Model Training Configuration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    " %%time\n",
    "# Defining model training function...\n",
    "def fit_model(X_train, y_train, X_val, y_val, model, run = 0):\n",
    "    '''\n",
    "    '''\n",
    "    lr_start = 0.01\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    epochs = EPOCHS    \n",
    "    lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, patience = 4, verbose = VERBOSE)\n",
    "    es = EarlyStopping(monitor = 'val_loss',patience = 12, verbose = 1, mode = 'min', restore_best_weights = True)\n",
    "    tm = tf.keras.callbacks.TerminateOnNaN()\n",
    "    callbacks = [lr, es, tm]\n",
    "    \n",
    "    # Cosine Learning Rate Decay\n",
    "    if USE_PLATEAU == False:\n",
    "        epochs = EPOCHS_COSINEDECAY\n",
    "        lr_end = 0.0002\n",
    "\n",
    "        def cosine_decay(epoch):\n",
    "            if epochs > 1:\n",
    "                w = (1 + math.cos(epoch / (epochs - 1) * math.pi)) / 2\n",
    "            else:\n",
    "                w = 1\n",
    "            return w * lr_start + (1 - w) * lr_end\n",
    "        \n",
    "        lr = LearningRateScheduler(cosine_decay, verbose = 0)\n",
    "        callbacks = [lr, tm]\n",
    "    \n",
    "    # Model Initialization...\n",
    "    #model = nn_model(features)\n",
    "    optimizer_func = tf.keras.optimizers.Adam(learning_rate = lr_start)\n",
    "    loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(optimizer = optimizer_func, loss = loss_func)\n",
    "    \n",
    "    \n",
    "    X_val = scaler.transform(X_val)\n",
    "    validation_data = (X_val, y_val)\n",
    "    \n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data = validation_data, \n",
    "                        epochs          = epochs,\n",
    "                        verbose         = VERBOSE,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        shuffle         = True,\n",
    "                        callbacks       = callbacks\n",
    "                       )\n",
    "    print(\"Model fitted\")\n",
    "    history_list = [history.history]\n",
    "    \n",
    "    print(f'Training Loss: {history_list[-1][\"loss\"][-1]:.5f}, Validation Loss: {history_list[-1][\"val_loss\"][-1]:.5f}')\n",
    "    callbacks, es, lr, tm, history = None, None, None, None, None\n",
    "    \n",
    "    \n",
    "    y_val_pred = model.predict(X_val, batch_size = BATCH_SIZE, verbose = VERBOSE).ravel()\n",
    "    amex_score = amex_metric(y_val.values, y_val_pred, return_components = False)\n",
    "    \n",
    "    print(f'Fold {run} | {str(datetime.datetime.now() - start_time)[-12:-7]}'\n",
    "          f'| Amex Score: {amex_score:.5f}')\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    #score_list.append(amex_score)\n",
    "    \n",
    "    #tst_data_scaled = scaler.transform(tst_agg_data[features])\n",
    "    #tst_pred = model.predict(tst_data_scaled)\n",
    "    #predictions.append(tst_pred)\n",
    "    print(amex_score)\n",
    "    return amex_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 2 µs, total: 9 µs\n",
      "Wall time: 13.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def nn_model_3(features):\n",
    "    regularization = 4e-4\n",
    "    activation = 'swish'\n",
    "    inputs = Input(shape = (len(features)))\n",
    "    x = Reshape((len(features), 1))(inputs)\n",
    "    # 15 agg features per main feature, size = 15, step = 15.\n",
    "    x = Conv1D(24,15,strides=15, activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(12,1, activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv1D(4,1, activation=activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.33)(x)\n",
    "    x = Dense(32, activation = activation)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(16, activation = activation)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Creating a Model Training Loop and Cross Validating in 5 Folds... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import math\n",
    "def train(trn_agg_data, features, model):\n",
    "    score_list = []\n",
    "    kf = KFold(n_splits = 5)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(trn_agg_data)):\n",
    "        X_train, X_val = trn_agg_data.iloc[trn_idx][features], trn_agg_data.iloc[val_idx][features]\n",
    "        y_train, y_val = trn_agg_data.iloc[trn_idx][TARGET], trn_agg_data.iloc[val_idx][TARGET]\n",
    "        print(\"Fold\",fold)\n",
    "        score_list.append(fit_model(X_train, y_train, X_val, y_val, model))\n",
    "    current_score = np.mean(score_list)\n",
    "    print(f'OOF AUC: {current_score:.5f}')\n",
    "    return current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i13:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "                          _i12:  1.3 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                          _i11:  1.1 KiB\n",
      "                StandardScaler:  1.0 KiB\n",
      "           QuantileTransformer:  1.0 KiB\n",
      "                 OneHotEncoder:  1.0 KiB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 s, sys: 3.84 s, total: 19 s\n",
      "Wall time: 12.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import math\n",
    "\n",
    "# Create empty lists to store NN information...\n",
    "best_fill_type = [3, 2, 0, 3, 0]\n",
    "trn_agg_data, tst_agg_data, features = get_data(best_fill_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 02:52:46.669704: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 02:52:50.429918: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted\n",
      "Training Loss: 0.20254, Validation Loss: 0.24208\n",
      "Fold 0 | 03:08| Amex Score: 0.77130\n",
      "\n",
      "0.7713018209194233\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.19812, Validation Loss: 0.24463\n",
      "Fold 0 | 03:26| Amex Score: 0.76629\n",
      "\n",
      "0.7662860480866411\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.19556, Validation Loss: 0.24391\n",
      "Fold 0 | 03:12| Amex Score: 0.77221\n",
      "\n",
      "0.7722149691568433\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.19551, Validation Loss: 0.24067\n",
      "Fold 0 | 03:05| Amex Score: 0.77895\n",
      "\n",
      "0.7789469027272286\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.19435, Validation Loss: 0.24153\n",
      "Fold 0 | 03:06| Amex Score: 0.77840\n",
      "\n",
      "0.7784039133895067\n",
      "OOF AUC: 0.77343\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.21868, Validation Loss: 0.23082\n",
      "Fold 0 | 02:59| Amex Score: 0.78078\n",
      "\n",
      "0.7807774892021924\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.21866, Validation Loss: 0.23147\n",
      "Fold 0 | 03:01| Amex Score: 0.77791\n",
      "\n",
      "0.7779113446958514\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.21853, Validation Loss: 0.22996\n",
      "Fold 0 | 03:02| Amex Score: 0.78166\n",
      "\n",
      "0.7816575241922673\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.21816, Validation Loss: 0.22905\n",
      "Fold 0 | 03:04| Amex Score: 0.77989\n",
      "\n",
      "0.779891118093035\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.21921, Validation Loss: 0.22799\n",
      "Fold 0 | 03:05| Amex Score: 0.78478\n",
      "\n",
      "0.784775664300178\n",
      "OOF AUC: 0.78100\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "for l2 in np.logspace(-6, -5, 2):\n",
    "    score_list.append(train(trn_agg_data, features, nn_model(features, l2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7734307308559286, 0.7810026280967047]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e-06, 1.e-05])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-6, -5, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib as plt\n",
    "def plot_history(history, *, n_epochs=None, plot_lr=False, title=None, bottom=None, top=None):\n",
    "    \"\"\"Plot (the last n_epochs epochs of) the training history\n",
    "    \n",
    "    Plots loss and optionally val_loss and lr.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    from_epoch = 0 if n_epochs is None else max(len(history['loss']) - n_epochs, 0)\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.plot(np.arange(from_epoch, len(history['loss'])), history['loss'][from_epoch:], label='Training loss')\n",
    "    try:\n",
    "        plt.plot(np.arange(from_epoch, len(history['loss'])), history['val_loss'][from_epoch:], label='Validation loss')\n",
    "        best_epoch = np.argmin(np.array(history['val_loss']))\n",
    "        best_val_loss = history['val_loss'][best_epoch]\n",
    "        if best_epoch >= from_epoch:\n",
    "            plt.scatter([best_epoch], [best_val_loss], c='r', label=f'Best val_loss = {best_val_loss:.5f}')\n",
    "        if best_epoch > 0:\n",
    "            almost_epoch = np.argmin(np.array(history['val_loss'])[:best_epoch])\n",
    "            almost_val_loss = history['val_loss'][almost_epoch]\n",
    "            if almost_epoch >= from_epoch:\n",
    "                plt.scatter([almost_epoch], [almost_val_loss], c='orange', label='Second best val_loss')\n",
    "    except KeyError:\n",
    "        pass\n",
    "    if bottom is not None: plt.ylim(bottom=bottom)\n",
    "    if top is not None: plt.ylim(top=top)\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='lower left')\n",
    "    if title is not None: plt.title(title)\n",
    "        \n",
    "    # Plot learning rate\n",
    "    if plot_lr and 'lr' in history:\n",
    "        ax2 = plt.gca().twinx()\n",
    "        ax2.plot(np.arange(from_epoch, len(history['lr'])), np.array(history['lr'][from_epoch:]), color='g', label='Learning rate')\n",
    "        ax2.set_ylabel('Learning rate')\n",
    "        ax2.legend(loc='upper right')\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3259/1076683167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot training history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m plot_history(model, \n\u001b[0m\u001b[1;32m      3\u001b[0m              \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Learning curve\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              plot_lr=True)\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "plot_history(model, \n",
    "             title=f\"Learning curve\",\n",
    "             plot_lr=True)\n",
    "\n",
    "# # Plot prediction histogram\n",
    "# plt.figure(figsize=(16, 5))\n",
    "# plt.hist(y_va_pred[y_va == 0], bins=np.linspace(0, 1, 21),\n",
    "#          alpha=0.5, density=True)\n",
    "# plt.hist(y_va_pred[y_va == 1], bins=np.linspace(0, 1, 21),\n",
    "#          alpha=0.5, density=True)\n",
    "# plt.xlabel('y_pred')\n",
    "# plt.ylabel('density')\n",
    "# plt.title('OOF Prediction Histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7783780391771623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7768925641517128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.0 Model Prediction and Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub['prediction'] = np.array(predictions).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub.to_csv('my_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
