{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### American Express - Default Prediction \n",
    "## Predict If A Customer Will Default in the Future ...\n",
    "The objective of this competition is to predict the probability that a customer does not pay back their credit card balance amount in the future based on their monthly customer profile. The target binary variable is calculated by observing 18 months performance window after the latest credit card statement, and if the customer does not pay due amount in 120 days after their latest statement date it is considered a default event.\n",
    "\n",
    "<img style=\"float: center;\" src=\"https://img.freepik.com/free-vector/brain-with-digital-circuit-programmer-with-laptop-machine-learning-artificial-intelligence-digital-brain-artificial-thinking-process-concept-vector-isolated-illustration_335657-2246.jpg?w=2000\" width = '550'>\n",
    "<a href='https://www.freepik.com/vectors/machine-learning'>Machine learning vector created by vectorjuice - www.freepik.com</a>\n",
    "\n",
    "#### Data Description\n",
    "The dataset contains aggregated profile features for each customer at each statement date. Features are anonymized and normalized, and fall into the following general categories:\n",
    "\n",
    "* D_* = Delinquency variables\n",
    "* S_* = Spend variables\n",
    "* P_* = Payment variables\n",
    "* B_* = Balance variables\n",
    "* R_* = Risk variables\n",
    "\n",
    "With the following features being categorical:\n",
    "\n",
    "**['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']**\n",
    "\n",
    "Your task is to predict, for each customer_ID, the probability of a future payment default (target = 1).\n",
    "\n",
    "Note that the negative class has been subsampled for this dataset at 5%, and thus receives a 20x weighting in the scoring metric.\n",
    "\n",
    "**Files**\n",
    "* train_data.csv - training data with multiple statement dates per customer_ID\n",
    "* train_labels.csv - target label for each customer_ID\n",
    "* test_data.csv - corresponding test data; your objective is to predict the target label for each customer_ID\n",
    "* sample_submission.csv - a sample submission file in the correct format\n",
    "\n",
    "---\n",
    "\n",
    "## My Strategy, or How I Will Aproach this Competition...\n",
    "We have data from many Customers and there is many points of information by for each of the customers, the target labels are only one per customer id so aggregation will be requiered, from here there is quie a lot of possibilities, this is what I will folow in this Notebook...\n",
    "\n",
    "#### Loading the Datasets\n",
    "The datasets is massive so I will rely on other Kaggles optimized datasets stored in a feather format to make my life easier in this competition.\n",
    "\n",
    "#### Quick EDA\n",
    "The typical analysis that I always like to complete to undertstand the dataset better...\n",
    "* Information of the datasets, size and others.\n",
    "* Simple visualization of the first few records.\n",
    "* Data statistical analalysis using describe.\n",
    "* Visualization of the number of NaNs.\n",
    "* Understanding the amount of unique records.\n",
    "\n",
    "#### Exploring the Target Variable\n",
    "Nothing in particular dataset seems to be quite inbalanced so I will get back to this part later...\n",
    "\n",
    "#### Structuring the Datasets\n",
    "Here is where everything happens, because we have time-base data o multiple points per customer we are trying to aggregate the information in certain way that's practical:\n",
    "* Statistical aggregation for numeric features\n",
    "* Only keep the last know record for analysis\n",
    "* Statictical aggregation for categorical features\n",
    "\n",
    "#### Feature Engineering\n",
    "At this point the only thing that I can consider some type of feature will be the aggregation of the datasets, as I mentioned in the previous point\n",
    "* Statistical aggregation\n",
    "* Only keep the last know record for analysis\n",
    "\n",
    "#### Label Encoding\n",
    "Because there is quite a lot of categorical variables and this is a NN model I will use the following encoding technique:\n",
    "* OneHot encoder, only train in the train dataset and applyed on test\n",
    "\n",
    "#### Fill NaNs**\n",
    "At this point just to get started, I will fill everything with ceros, probably not a good idea.\n",
    "* Fill NaNs with 0\n",
    "\n",
    "#### Model Development and Training\n",
    "I'm going to go first with an NN in the last few competitions the NN models have been working quite well also we have so much data.\n",
    "* Simple NN tested, layer after later.\n",
    "* I also tested a more complex NN, that I learned from Ambross with Skip conections.\n",
    "\n",
    "#### Predictions and Submission\n",
    "No much details here, just the simple average of all the predictions across multiple folds.\n",
    "* Average predictions across 5 folds\n",
    "\n",
    "---\n",
    "\n",
    "## Updates\n",
    "#### 05/28/2022\n",
    "* Build the initial model using Neuronal Nets and simple agg strategy (Last data point).\n",
    "* Evaluated the model and uploaded for Ranking.\n",
    "\n",
    "#### 05/29/2022\n",
    "* Improve model architecture.\n",
    "* Really dive deep into Feature Engineering (Not much here, memory is a big challenge)\n",
    "\n",
    "#### 05/30/2022\n",
    "* ...\n",
    "\n",
    "---\n",
    "\n",
    "## Resources, Inspiration\n",
    "I have taken Ideas or learned quite a lot from the Notebooks below, please check also if you like my work.\n",
    "\n",
    "* https://www.kaggle.com/code/ambrosm/amex-keras-quickstart-1-training/notebook\n",
    "* ...\n",
    "* ...\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Loading Model Libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/amex-default-prediction/train_labels.csv\n",
      "/kaggle/input/parquet-files-amexdefault-prediction/train_data.ftr\n",
      "/kaggle/input/parquet-files-amexdefault-prediction/test_data.ftr\n",
      "CPU times: user 555 µs, sys: 81 µs, total: 636 µs\n",
      "Wall time: 460 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 1 µs, total: 8 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import datetime # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Setting the Notebook Parameters and Default Configuration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33 µs, sys: 5 µs, total: 38 µs\n",
      "Wall time: 43.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# I like to disable my Notebook Warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Notebook Configuration...\n",
    "\n",
    "# Amount of data we want to load into the Model...\n",
    "DATA_ROWS = None\n",
    "# Dataframe, the amount of rows and cols to visualize...\n",
    "NROWS = 50\n",
    "NCOLS = 15\n",
    "# Main data location path...\n",
    "BASE_PATH = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 104 µs, sys: 15 µs, total: 119 µs\n",
      "Wall time: 128 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Configure notebook display settings to only use 2 decimal places, tables look nicer.\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_columns', NCOLS) \n",
    "pd.set_option('display.max_rows', NROWS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Loading the Dataset Information (Using Feather)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer, OneHotEncoder, OrdinalEncoder\n",
    "def get_data(fill_values, get_test=False):\n",
    "    # Load the CSV information into a Pandas DataFrame...\n",
    "    trn_data = pd.read_feather('../input/parquet-files-amexdefault-prediction/train_data.ftr')\n",
    "    trn_lbls = pd.read_csv('/kaggle/input/amex-default-prediction/train_labels.csv').set_index('customer_ID')\n",
    "    if(get_test):\n",
    "        tst_data = pd.read_feather('../input/parquet-files-amexdefault-prediction/test_data.ftr')\n",
    "\n",
    "    #%%time\n",
    "    #sub = pd.read_csv('/kaggle/input/amex-default-prediction/sample_submission.csv')\n",
    "\n",
    "    ## 6.1 Training Dataset...\n",
    "\n",
    "    # We have 458913 customers. and we have 458913 train labels...\n",
    "\n",
    "    # Calculates the amount of information by costumer or records available...\n",
    "    trn_num_statements = trn_data.groupby('customer_ID').size().sort_index()\n",
    "\n",
    "    # Create a new dataset based on aggregated information\n",
    "    trn_agg_data = (trn_data\n",
    "                    .groupby('customer_ID')\n",
    "                    .tail(1)\n",
    "                    .set_index('customer_ID', drop=True)\n",
    "                    .sort_index()\n",
    "                    .drop(['S_2'], axis='columns'))\n",
    "    del trn_data\n",
    "    # Merge the labels from the labels dataframe\n",
    "    trn_agg_data['target'] = trn_lbls.target\n",
    "    del trn_lbls\n",
    "    trn_agg_data['num_statements'] = trn_num_statements\n",
    "    del trn_num_statements\n",
    "    \n",
    "    trn_agg_data.reset_index(inplace = True, drop = True) # forget the customer_IDs\n",
    "\n",
    "    ## 6.2 Test Dataset...\n",
    "\n",
    "    # Calculates the amount of information by costumer or records available...\n",
    "    if(get_test):\n",
    "        tst_num_statements = tst_data.groupby('customer_ID').size().sort_index()\n",
    "\n",
    "        # Create a new dataset based on aggregated information\n",
    "        tst_agg_data = (tst_data\n",
    "                        .groupby('customer_ID')\n",
    "                        .tail(1)\n",
    "                        .set_index('customer_ID', drop=True)\n",
    "                        .sort_index()\n",
    "                        .drop(['S_2'], axis='columns'))\n",
    "        del tst_data\n",
    "        # Merge the labels from the labels dataframe\n",
    "        tst_agg_data['num_statements'] = tst_num_statements\n",
    "        del tst_num_statements\n",
    "        tst_agg_data.reset_index(inplace = True, drop = True) # forget the customer_IDs\n",
    "\n",
    "    # 7.0 Label / One-Hot Encoding the Categorical Variables...\n",
    "\n",
    "    ## 7.1 One Hot Encoding Configuration...\n",
    "\n",
    "    # One-hot Encoding Configuration\n",
    "    cat_features = ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']\n",
    "\n",
    "    #trn_agg_data[cat_features] = trn_agg_data[cat_features].astype(object)\n",
    "    trn_not_cat_features = [f for f in trn_agg_data.columns if f not in cat_features]\n",
    "    if(get_test):\n",
    "        tst_not_cat_features = [f for f in tst_agg_data.columns if f not in cat_features]\n",
    "\n",
    "    #encoder = OneHotEncoder(drop = 'first', sparse = False, dtype = np.float32, handle_unknown = 'ignore')\n",
    "    encoder = OrdinalEncoder()\n",
    "    trn_encoded_features = encoder.fit_transform(trn_agg_data[cat_features])\n",
    "    #feat_names = list(encoder.get_feature_names())\n",
    "\n",
    "    ## 7.2 Train Dataset One Hot Encoding...\n",
    "\n",
    "    # One-hot Encoding\n",
    "    trn_encoded_features = pd.DataFrame(trn_encoded_features)\n",
    "    #trn_encoded_features.columns = feat_names\n",
    "\n",
    "    trn_agg_data = pd.concat([trn_agg_data[trn_not_cat_features], trn_encoded_features], axis = 1)\n",
    "\n",
    "    ## 7.3 Test Dataset One-Hot Encoding...\n",
    "    if(get_test):\n",
    "        # One-hot Encoding\n",
    "        tst_encoded_features = encoder.transform(tst_agg_data[cat_features])\n",
    "        tst_encoded_features = pd.DataFrame(tst_encoded_features)\n",
    "        #tst_encoded_features.columns = feat_names\n",
    "\n",
    "        tst_agg_data = pd.concat([tst_agg_data[tst_not_cat_features], tst_encoded_features], axis = 1)\n",
    "        tst_agg_data.head()\n",
    "\n",
    "    features = [f for f in trn_agg_data.columns if f != 'target' and f != 'customer_ID']\n",
    "    \n",
    "    c = trn_agg_data[features].columns.str\n",
    "    cs = [c.startswith('S_', False), c.startswith('P_', False), c.startswith('B_', False), c.startswith('R_', False), c.startswith('D_', False)]\n",
    "    cs = [trn_agg_data[features].columns[c_i] for c_i in cs]\n",
    "    #\n",
    "    # Impute missing values\n",
    "    # Old fill type values\n",
    "    for i_fill in range(len(fill_values)):\n",
    "        if(fill_values[i_fill]==0):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = 0, inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = 0, inplace = True)\n",
    "        elif(fill_values[i_fill]==1):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = np.nanmean(trn_agg_data[cs[i_fill]]), inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = np.nanmean(trn_agg_data[cs[i_fill]]), inplace = True)         \n",
    "        elif(fill_values[i_fill]==2):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .25), inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .25), inplace = True)       \n",
    "        elif(fill_values[i_fill]==3):\n",
    "            trn_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .75), inplace = True)\n",
    "            if(get_test):\n",
    "                tst_agg_data[cs[i_fill]].fillna(value = np.nanquantile(trn_agg_data[cs[i_fill]], .75), inplace = True)            \n",
    "    #Fill all others\n",
    "    trn_agg_data.fillna(value = 0, inplace = True)\n",
    "    if(get_test):\n",
    "        tst_agg_data.fillna(value = 0, inplace = True)\n",
    "\n",
    "    # 10.0 NN Development\n",
    "\n",
    "    # Release some memory by deleting the original DataFrames...\n",
    "    gc.collect()\n",
    "    if(get_test==False):\n",
    "        tst_agg_data = 0\n",
    "    return trn_agg_data, tst_agg_data, features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Loading Specific Model Libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.49 s, sys: 251 ms, total: 1.74 s\n",
      "Wall time: 1.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, LearningRateScheduler, EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Input, InputLayer, Add, BatchNormalization, Dropout, Concatenate\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Amex Metric, Function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 1e+03 ns, total: 7 µs\n",
      "Wall time: 12.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# From https://www.kaggle.com/code/inversion/amex-competition-metric-python\n",
    "\n",
    "def amex_metric(y_true, y_pred, return_components=False) -> float:\n",
    "    \"\"\"Amex metric for ndarrays\"\"\"\n",
    "    \n",
    "    def top_four_percent_captured(df) -> float:\n",
    "        \"\"\"Corresponds to the recall for a threshold of 4 %\"\"\"\n",
    "        \n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        four_pct_cutoff = int(0.04 * df['weight'].sum())\n",
    "        df['weight_cumsum'] = df['weight'].cumsum()\n",
    "        df_cutoff = df.loc[df['weight_cumsum'] <= four_pct_cutoff]\n",
    "        return (df_cutoff['target'] == 1).sum() / (df['target'] == 1).sum()\n",
    "    \n",
    "    \n",
    "    def weighted_gini(df) -> float:\n",
    "        df['weight'] = df['target'].apply(lambda x: 20 if x==0 else 1)\n",
    "        df['random'] = (df['weight'] / df['weight'].sum()).cumsum()\n",
    "        total_pos = (df['target'] * df['weight']).sum()\n",
    "        df['cum_pos_found'] = (df['target'] * df['weight']).cumsum()\n",
    "        df['lorentz'] = df['cum_pos_found'] / total_pos\n",
    "        df['gini'] = (df['lorentz'] - df['random']) * df['weight']\n",
    "        return df['gini'].sum()\n",
    "\n",
    "    \n",
    "    def normalized_weighted_gini(df) -> float:\n",
    "        \"\"\"Corresponds to 2 * AUC - 1\"\"\"\n",
    "        \n",
    "        df2 = pd.DataFrame({'target': df.target, 'prediction': df.target})\n",
    "        df2.sort_values('prediction', ascending=False, inplace=True)\n",
    "        return weighted_gini(df) / weighted_gini(df2)\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame({'target': y_true.ravel(), 'prediction': y_pred.ravel()})\n",
    "    df.sort_values('prediction', ascending=False, inplace=True)\n",
    "    g = normalized_weighted_gini(df)\n",
    "    d = top_four_percent_captured(df)\n",
    "\n",
    "    if return_components: return g, d, 0.5 * (g + d)\n",
    "    return 0.5 * (g + d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Defining the NN Model Architecture..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.1 Architecture 01, Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def nn_model():\n",
    "    '''\n",
    "    '''\n",
    "    regularization = 4e-4\n",
    "    activation_func = 'swish'\n",
    "    inputs = Input(shape = (len(features)))\n",
    "    \n",
    "    x = Dense(256, \n",
    "              #use_bias  = True, \n",
    "              kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "              activation = activation_func)(inputs)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(64, \n",
    "              #use_bias  = True, \n",
    "              kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "              activation = activation_func)(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Dense(64, \n",
    "          #use_bias  = True, \n",
    "          kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "          activation = activation_func)(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(32, \n",
    "              #use_bias  = True, \n",
    "              kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "              activation = activation_func)(x)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Dense(1, \n",
    "              #use_bias  = True, \n",
    "              #kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "              activation = 'sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs, x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3.2 Architecture 02, Concatenated NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
      "Wall time: 12.9 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def nn_model(features):\n",
    "    regularization = 4e-4\n",
    "    activation_func = 'swish'\n",
    "    inputs = Input(shape = (len(features)))\n",
    "\n",
    "    x0 = Dense(256,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization), \n",
    "               activation = activation_func)(inputs)\n",
    "    x1 = Dense(128,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "               activation = activation_func)(x0)\n",
    "    x1 = Dense(64,\n",
    "               kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "               activation = activation_func)(x1)\n",
    "    x1 = Dense(32,\n",
    "           kernel_regularizer = tf.keras.regularizers.l2(regularization),\n",
    "           activation = activation_func)(x1)\n",
    "    \n",
    "    x1 = Concatenate()([x1, x0])\n",
    "    x1 = Dropout(0.1)(x1)\n",
    "    \n",
    "    x1 = Dense(16, kernel_regularizer=tf.keras.regularizers.l2(regularization),activation=activation_func,)(x1)\n",
    "    \n",
    "    x1 = Dense(1, \n",
    "              #kernel_regularizer=tf.keras.regularizers.l2(regularization),\n",
    "              activation='sigmoid')(x1)\n",
    "    \n",
    "    model = Model(inputs, x1)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Defining Model Training Parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1e+03 ns, total: 6 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Defining model parameters...\n",
    "BATCH_SIZE         = 256\n",
    "EPOCHS             = 20 \n",
    "EPOCHS_COSINEDECAY = 20\n",
    "DIAGRAMS           = True\n",
    "USE_PLATEAU        = False\n",
    "INFERENCE          = False\n",
    "VERBOSE            = 0 \n",
    "TARGET             = 'target'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Defining the Model Training Configuration..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 µs, sys: 2 µs, total: 8 µs\n",
      "Wall time: 14.1 µs\n"
     ]
    }
   ],
   "source": [
    " %%time\n",
    "# Defining model training function...\n",
    "def fit_model(X_train, y_train, X_val, y_val, run = 0):\n",
    "    '''\n",
    "    '''\n",
    "    lr_start = 0.01\n",
    "    start_time = datetime.datetime.now()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    epochs = EPOCHS    \n",
    "    lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.7, patience = 4, verbose = VERBOSE)\n",
    "    es = EarlyStopping(monitor = 'val_loss',patience = 12, verbose = 1, mode = 'min', restore_best_weights = True)\n",
    "    tm = tf.keras.callbacks.TerminateOnNaN()\n",
    "    callbacks = [lr, es, tm]\n",
    "    \n",
    "    # Cosine Learning Rate Decay\n",
    "    if USE_PLATEAU == False:\n",
    "        epochs = EPOCHS_COSINEDECAY\n",
    "        lr_end = 0.0002\n",
    "\n",
    "        def cosine_decay(epoch):\n",
    "            if epochs > 1:\n",
    "                w = (1 + math.cos(epoch / (epochs - 1) * math.pi)) / 2\n",
    "            else:\n",
    "                w = 1\n",
    "            return w * lr_start + (1 - w) * lr_end\n",
    "        \n",
    "        lr = LearningRateScheduler(cosine_decay, verbose = 0)\n",
    "        callbacks = [lr, tm]\n",
    "    \n",
    "    # Model Initialization...\n",
    "    model = nn_model(features)\n",
    "    optimizer_func = tf.keras.optimizers.Adam(learning_rate = lr_start)\n",
    "    loss_func = tf.keras.losses.BinaryCrossentropy()\n",
    "    model.compile(optimizer = optimizer_func, loss = loss_func)\n",
    "    \n",
    "    \n",
    "    X_val = scaler.transform(X_val)\n",
    "    validation_data = (X_val, y_val)\n",
    "    \n",
    "    history = model.fit(X_train, \n",
    "                        y_train, \n",
    "                        validation_data = validation_data, \n",
    "                        epochs          = epochs,\n",
    "                        verbose         = VERBOSE,\n",
    "                        batch_size      = BATCH_SIZE,\n",
    "                        shuffle         = True,\n",
    "                        callbacks       = callbacks\n",
    "                       )\n",
    "    print(\"Model fitted\")\n",
    "    history_list = [history.history]\n",
    "    \n",
    "    print(f'Training Loss: {history_list[-1][\"loss\"][-1]:.5f}, Validation Loss: {history_list[-1][\"val_loss\"][-1]:.5f}')\n",
    "    callbacks, es, lr, tm, history = None, None, None, None, None\n",
    "    \n",
    "    \n",
    "    y_val_pred = model.predict(X_val, batch_size = BATCH_SIZE, verbose = VERBOSE).ravel()\n",
    "    amex_score = amex_metric(y_val.values, y_val_pred, return_components = False)\n",
    "    \n",
    "    print(f'Fold {run} | {str(datetime.datetime.now() - start_time)[-12:-7]}'\n",
    "          f'| Amex Score: {amex_score:.5f}')\n",
    "    \n",
    "    print('')\n",
    "    \n",
    "    #score_list.append(amex_score)\n",
    "    \n",
    "    #tst_data_scaled = scaler.transform(tst_agg_data[features])\n",
    "    #tst_pred = model.predict(tst_data_scaled)\n",
    "    #predictions.append(tst_pred)\n",
    "    print(amex_score)\n",
    "    return amex_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.7 Creating a Model Training Loop and Cross Validating in 5 Folds... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import math\n",
    "def train(trn_agg_data, features):\n",
    "    score_list = []\n",
    "    kf = KFold(n_splits = 5)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(kf.split(trn_agg_data)):\n",
    "        X_train, X_val = trn_agg_data.iloc[trn_idx][features], trn_agg_data.iloc[val_idx][features]\n",
    "        y_train, y_val = trn_agg_data.iloc[trn_idx][TARGET], trn_agg_data.iloc[val_idx][TARGET]\n",
    "        print(\"Fold\",fold)\n",
    "        score_list.append(fit_model(X_train, y_train, X_val, y_val))\n",
    "    current_score = np.mean(score_list)\n",
    "    print(f'OOF AUC: {current_score:.5f}')\n",
    "    return current_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           _i6:  5.6 KiB\n",
      "                           _ii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                          _i10:  1.1 KiB\n",
      "                StandardScaler:  1.0 KiB\n",
      "           QuantileTransformer:  1.0 KiB\n",
      "                 OneHotEncoder:  1.0 KiB\n",
      "                OrdinalEncoder:  1.0 KiB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-09 15:16:51.346500: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-08-09 15:16:52.056823: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fitted\n",
      "Training Loss: 0.23066, Validation Loss: 0.23230\n",
      "Fold 0 | 03:05| Amex Score: 0.77791\n",
      "\n",
      "0.7779084630048432\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23075, Validation Loss: 0.23274\n",
      "Fold 0 | 02:57| Amex Score: 0.77708\n",
      "\n",
      "0.7770769957549349\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23120, Validation Loss: 0.23025\n",
      "Fold 0 | 02:58| Amex Score: 0.77950\n",
      "\n",
      "0.7795020010305063\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23179, Validation Loss: 0.22933\n",
      "Fold 0 | 02:57| Amex Score: 0.78017\n",
      "\n",
      "0.7801666138912626\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23158, Validation Loss: 0.22857\n",
      "Fold 0 | 02:57| Amex Score: 0.78102\n",
      "\n",
      "0.7810156232080048\n",
      "OOF AUC: 0.77913\n",
      "Feature type 0\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23082, Validation Loss: 0.23274\n",
      "Fold 0 | 02:57| Amex Score: 0.77730\n",
      "\n",
      "0.7773020120007867\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23065, Validation Loss: 0.23221\n",
      "Fold 0 | 02:57| Amex Score: 0.77728\n",
      "\n",
      "0.7772838370296578\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23102, Validation Loss: 0.22999\n",
      "Fold 0 | 02:57| Amex Score: 0.78107\n",
      "\n",
      "0.7810660812703685\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23140, Validation Loss: 0.22922\n",
      "Fold 0 | 02:56| Amex Score: 0.77945\n",
      "\n",
      "0.7794516727511492\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23186, Validation Loss: 0.22868\n",
      "Fold 0 | 03:26| Amex Score: 0.78082\n",
      "\n",
      "0.7808234781349681\n",
      "OOF AUC: 0.77919\n",
      "Best fill type list is now [1 0 0 0 0]\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23061, Validation Loss: 0.23265\n",
      "Fold 0 | 02:59| Amex Score: 0.77718\n",
      "\n",
      "0.7771808881695845\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23063, Validation Loss: 0.23238\n",
      "Fold 0 | 03:00| Amex Score: 0.77924\n",
      "\n",
      "0.7792431739264405\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23111, Validation Loss: 0.23030\n",
      "Fold 0 | 03:26| Amex Score: 0.78002\n",
      "\n",
      "0.7800234644785157\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23156, Validation Loss: 0.22912\n",
      "Fold 0 | 02:58| Amex Score: 0.78058\n",
      "\n",
      "0.7805785921706284\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23176, Validation Loss: 0.22859\n",
      "Fold 0 | 02:58| Amex Score: 0.78163\n",
      "\n",
      "0.7816289021846907\n",
      "OOF AUC: 0.77973\n",
      "Best fill type list is now [2 0 0 0 0]\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23070, Validation Loss: 0.23256\n",
      "Fold 0 | 02:57| Amex Score: 0.77722\n",
      "\n",
      "0.7772245703549083\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23089, Validation Loss: 0.23258\n",
      "Fold 0 | 02:56| Amex Score: 0.77873\n",
      "\n",
      "0.7787264669364509\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23138, Validation Loss: 0.23022\n",
      "Fold 0 | 03:00| Amex Score: 0.78075\n",
      "\n",
      "0.7807459423984986\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23116, Validation Loss: 0.22851\n",
      "Fold 0 | 03:01| Amex Score: 0.78041\n",
      "\n",
      "0.7804062819666413\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23233, Validation Loss: 0.22874\n",
      "Fold 0 | 02:58| Amex Score: 0.78182\n",
      "\n",
      "0.7818208561082822\n",
      "OOF AUC: 0.77978\n",
      "Best fill type list is now [3 0 0 0 0]\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "Feature type 1\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23081, Validation Loss: 0.23257\n",
      "Fold 0 | 02:58| Amex Score: 0.77824\n",
      "\n",
      "0.7782423306909461\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23056, Validation Loss: 0.23255\n",
      "Fold 0 | 02:56| Amex Score: 0.77759\n",
      "\n",
      "0.7775854659332935\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23142, Validation Loss: 0.23005\n",
      "Fold 0 | 02:57| Amex Score: 0.77970\n",
      "\n",
      "0.7797014563682096\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23175, Validation Loss: 0.22876\n",
      "Fold 0 | 02:59| Amex Score: 0.78103\n",
      "\n",
      "0.7810272690458241\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23178, Validation Loss: 0.22864\n",
      "Fold 0 | 02:58| Amex Score: 0.78193\n",
      "\n",
      "0.7819325126402362\n",
      "OOF AUC: 0.77970\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23058, Validation Loss: 0.23262\n",
      "Fold 0 | 02:55| Amex Score: 0.77762\n",
      "\n",
      "0.7776165536006037\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23085, Validation Loss: 0.23242\n",
      "Fold 0 | 02:57| Amex Score: 0.77850\n",
      "\n",
      "0.7784980501580785\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23097, Validation Loss: 0.23000\n",
      "Fold 0 | 02:57| Amex Score: 0.78149\n",
      "\n",
      "0.7814944876123764\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23148, Validation Loss: 0.22854\n",
      "Fold 0 | 02:57| Amex Score: 0.78030\n",
      "\n",
      "0.780299190438585\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23164, Validation Loss: 0.22855\n",
      "Fold 0 | 02:57| Amex Score: 0.78297\n",
      "\n",
      "0.7829704982048364\n",
      "OOF AUC: 0.78018\n",
      "Best fill type list is now [3 2 0 0 0]\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23044, Validation Loss: 0.23254\n",
      "Fold 0 | 02:57| Amex Score: 0.77788\n",
      "\n",
      "0.7778772334877193\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23069, Validation Loss: 0.23250\n",
      "Fold 0 | 03:26| Amex Score: 0.77778\n",
      "\n",
      "0.7777758214158847\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23119, Validation Loss: 0.23032\n",
      "Fold 0 | 02:59| Amex Score: 0.78115\n",
      "\n",
      "0.7811480610660024\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23171, Validation Loss: 0.22853\n",
      "Fold 0 | 03:01| Amex Score: 0.78143\n",
      "\n",
      "0.7814273643730056\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23181, Validation Loss: 0.22849\n",
      "Fold 0 | 03:01| Amex Score: 0.78088\n",
      "\n",
      "0.7808813909562288\n",
      "OOF AUC: 0.77982\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "Feature type 2\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23074, Validation Loss: 0.23257\n",
      "Fold 0 | 02:57| Amex Score: 0.77793\n",
      "\n",
      "0.7779328431434864\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23105, Validation Loss: 0.23236\n",
      "Fold 0 | 03:03| Amex Score: 0.77729\n",
      "\n",
      "0.7772890224550277\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23131, Validation Loss: 0.23016\n",
      "Fold 0 | 03:00| Amex Score: 0.77973\n",
      "\n",
      "0.7797347269968378\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23176, Validation Loss: 0.22885\n",
      "Fold 0 | 03:26| Amex Score: 0.78117\n",
      "\n",
      "0.7811679178657265\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23186, Validation Loss: 0.22846\n",
      "Fold 0 | 02:59| Amex Score: 0.78223\n",
      "\n",
      "0.7822266163907277\n",
      "OOF AUC: 0.77967\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23046, Validation Loss: 0.23284\n",
      "Fold 0 | 02:56| Amex Score: 0.77897\n",
      "\n",
      "0.7789695022173939\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23067, Validation Loss: 0.23260\n",
      "Fold 0 | 03:26| Amex Score: 0.77705\n",
      "\n",
      "0.77705279772653\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23147, Validation Loss: 0.23074\n",
      "Fold 0 | 03:01| Amex Score: 0.78064\n",
      "\n",
      "0.780640359530675\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23163, Validation Loss: 0.22884\n",
      "Fold 0 | 02:58| Amex Score: 0.77979\n",
      "\n",
      "0.7797865097726088\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23173, Validation Loss: 0.22861\n",
      "Fold 0 | 02:59| Amex Score: 0.78196\n",
      "\n",
      "0.7819620595351462\n",
      "OOF AUC: 0.77968\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23052, Validation Loss: 0.23248\n",
      "Fold 0 | 03:26| Amex Score: 0.77765\n",
      "\n",
      "0.7776474551780226\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23087, Validation Loss: 0.23244\n",
      "Fold 0 | 02:58| Amex Score: 0.77879\n",
      "\n",
      "0.7787890682785372\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23121, Validation Loss: 0.23023\n",
      "Fold 0 | 02:59| Amex Score: 0.78050\n",
      "\n",
      "0.7805015217442235\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23180, Validation Loss: 0.22908\n",
      "Fold 0 | 02:57| Amex Score: 0.78070\n",
      "\n",
      "0.7806968843024306\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23172, Validation Loss: 0.22843\n",
      "Fold 0 | 02:59| Amex Score: 0.78265\n",
      "\n",
      "0.78265284170835\n",
      "OOF AUC: 0.78006\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "Feature type 3\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23061, Validation Loss: 0.23282\n",
      "Fold 0 | 02:59| Amex Score: 0.77734\n",
      "\n",
      "0.777341339532764\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23078, Validation Loss: 0.23221\n",
      "Fold 0 | 02:58| Amex Score: 0.77826\n",
      "\n",
      "0.7782553351869006\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23128, Validation Loss: 0.23007\n",
      "Fold 0 | 02:59| Amex Score: 0.77975\n",
      "\n",
      "0.779746188424929\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23167, Validation Loss: 0.22862\n",
      "Fold 0 | 03:00| Amex Score: 0.78114\n",
      "\n",
      "0.7811367631138436\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23186, Validation Loss: 0.22854\n",
      "Fold 0 | 02:57| Amex Score: 0.78299\n",
      "\n",
      "0.782985861702676\n",
      "OOF AUC: 0.77989\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23049, Validation Loss: 0.23267\n",
      "Fold 0 | 02:57| Amex Score: 0.77958\n",
      "\n",
      "0.7795783884817551\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23078, Validation Loss: 0.23221\n",
      "Fold 0 | 03:00| Amex Score: 0.77734\n",
      "\n",
      "0.7773448149035371\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23141, Validation Loss: 0.23031\n",
      "Fold 0 | 03:00| Amex Score: 0.78094\n",
      "\n",
      "0.7809411193806556\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23172, Validation Loss: 0.22893\n",
      "Fold 0 | 03:02| Amex Score: 0.77999\n",
      "\n",
      "0.7799905598726604\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23161, Validation Loss: 0.22851\n",
      "Fold 0 | 02:59| Amex Score: 0.78230\n",
      "\n",
      "0.7823032413854139\n",
      "OOF AUC: 0.78003\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23067, Validation Loss: 0.23246\n",
      "Fold 0 | 02:59| Amex Score: 0.77908\n",
      "\n",
      "0.779083577851605\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23058, Validation Loss: 0.23231\n",
      "Fold 0 | 02:59| Amex Score: 0.77760\n",
      "\n",
      "0.777598532790474\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23082, Validation Loss: 0.22999\n",
      "Fold 0 | 03:01| Amex Score: 0.78122\n",
      "\n",
      "0.7812195403446716\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23191, Validation Loss: 0.22894\n",
      "Fold 0 | 02:58| Amex Score: 0.78128\n",
      "\n",
      "0.7812756933172509\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23147, Validation Loss: 0.22841\n",
      "Fold 0 | 03:26| Amex Score: 0.78204\n",
      "\n",
      "0.7820436952149972\n",
      "OOF AUC: 0.78024\n",
      "Best fill type list is now [3 2 0 3 0]\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "Feature type 4\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23031, Validation Loss: 0.23263\n",
      "Fold 0 | 02:57| Amex Score: 0.77722\n",
      "\n",
      "0.7772194961662657\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23091, Validation Loss: 0.23235\n",
      "Fold 0 | 03:00| Amex Score: 0.77780\n",
      "\n",
      "0.7778006164895712\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23122, Validation Loss: 0.23017\n",
      "Fold 0 | 02:57| Amex Score: 0.78037\n",
      "\n",
      "0.7803662777701383\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23194, Validation Loss: 0.22877\n",
      "Fold 0 | 02:57| Amex Score: 0.78108\n",
      "\n",
      "0.7810819328222031\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23198, Validation Loss: 0.22866\n",
      "Fold 0 | 02:58| Amex Score: 0.78204\n",
      "\n",
      "0.7820428822543573\n",
      "OOF AUC: 0.77970\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23069, Validation Loss: 0.23265\n",
      "Fold 0 | 02:57| Amex Score: 0.77859\n",
      "\n",
      "0.7785916231734058\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23049, Validation Loss: 0.23226\n",
      "Fold 0 | 03:26| Amex Score: 0.77696\n",
      "\n",
      "0.7769634065406352\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23136, Validation Loss: 0.23052\n",
      "Fold 0 | 02:58| Amex Score: 0.78009\n",
      "\n",
      "0.7800868263313477\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23162, Validation Loss: 0.22873\n",
      "Fold 0 | 02:57| Amex Score: 0.78113\n",
      "\n",
      "0.7811309453937293\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23179, Validation Loss: 0.22852\n",
      "Fold 0 | 02:57| Amex Score: 0.78308\n",
      "\n",
      "0.7830828442943262\n",
      "OOF AUC: 0.77997\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "got data\n",
      "Fold 0\n",
      "Model fitted\n",
      "Training Loss: 0.23051, Validation Loss: 0.23240\n",
      "Fold 0 | 02:57| Amex Score: 0.77789\n",
      "\n",
      "0.7778890871107386\n",
      "Fold 1\n",
      "Model fitted\n",
      "Training Loss: 0.23060, Validation Loss: 0.23234\n",
      "Fold 0 | 02:59| Amex Score: 0.77795\n",
      "\n",
      "0.7779505758059948\n",
      "Fold 2\n",
      "Model fitted\n",
      "Training Loss: 0.23132, Validation Loss: 0.23024\n",
      "Fold 0 | 03:00| Amex Score: 0.78043\n",
      "\n",
      "0.7804336650308109\n",
      "Fold 3\n",
      "Model fitted\n",
      "Training Loss: 0.23140, Validation Loss: 0.22902\n",
      "Fold 0 | 02:58| Amex Score: 0.78014\n",
      "\n",
      "0.7801376242393496\n",
      "Fold 4\n",
      "Model fitted\n",
      "Training Loss: 0.23176, Validation Loss: 0.22852\n",
      "Fold 0 | 02:57| Amex Score: 0.78278\n",
      "\n",
      "0.782778500903587\n",
      "OOF AUC: 0.77984\n",
      "                           _i6:  5.6 KiB\n",
      "                          _iii:  2.6 KiB\n",
      "                          _i12:  2.6 KiB\n",
      "                           _i8:  1.7 KiB\n",
      "             ReduceLROnPlateau:  1.4 KiB\n",
      "                 EarlyStopping:  1.4 KiB\n",
      "                          _i15:  1.4 KiB\n",
      "                           _i9:  1.3 KiB\n",
      "                StandardScaler:  1.2 KiB\n",
      "                OrdinalEncoder:  1.2 KiB\n",
      "CPU times: user 6h 37min 7s, sys: 2h 9min 40s, total: 8h 46min 48s\n",
      "Wall time: 4h 7min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gc.collect()\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import math\n",
    "score_array = np.zeros((5,4))\n",
    "# Create empty lists to store NN information...\n",
    "best_fill_type = [0, 0, 0, 0, 0]\n",
    "trn_agg_data, tst_agg_data, features = get_data(best_fill_type)\n",
    "best_score = train(trn_agg_data, features)\n",
    "length = len(features)\n",
    "del trn_agg_data, tst_agg_data, features\n",
    "# Define kfolds for training purposes...\n",
    "for col in range(len(best_fill_type)):\n",
    "    print(\"Feature type\", col)\n",
    "    score_array[col,0] = best_score\n",
    "\n",
    "    best_fill_type_current = 0\n",
    "    for fill_type in range(1, 4):\n",
    "        new_fill = np.copy(best_fill_type)\n",
    "        new_fill[col] = fill_type\n",
    "        trn_agg_data, tst_agg_data, features = get_data(new_fill)\n",
    "        print(\"got data\")\n",
    "        current_score = train(trn_agg_data, features)\n",
    "        score_array[col, fill_type] = current_score\n",
    "        if(current_score > best_score):\n",
    "            best_fill_type = new_fill\n",
    "            best_score = current_score  \n",
    "            print(\"Best fill type list is now\", best_fill_type)\n",
    "        #free space\n",
    "        del trn_agg_data, tst_agg_data, features\n",
    "        gc.collect()\n",
    "        for name, size in sorted(((name, sys.getsizeof(value)) for name, value in globals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "            print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score array:\n",
      " [[0.77913394 0.77918542 0.779731   0.77978482]\n",
      " [0.77978482 0.77969781 0.78017576 0.77982197]\n",
      " [0.78017576 0.77967023 0.77968225 0.78005755]\n",
      " [0.78017576 0.7798931  0.78003162 0.78024421]\n",
      " [0.78024421 0.77970224 0.77997113 0.77983789]]\n",
      "Best score:  0.7802442079037997\n",
      "Best fill type: [3 2 0 3 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Score array:\\n\",score_array)\n",
    "print(\"Best score: \", best_score)\n",
    "print(\"Best fill type:\", best_fill_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.0 Model Prediction and Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sub' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub['prediction'] = np.array(predictions).mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sub' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub.to_csv('my_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sub' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
